# Fluentd + HDFS: Instant Big Data Collection

This article describes how to use [Fluentd](http://fluentd.org/)'s newly released [WebHDFS plugin](http://github.com/fluent/fluent-plugin-webhdfs/), to aggregate semi-structured logs into Hadoop HDFS.

## Background

[Fluentd](http://fluentd.org/) is an advanced open-source log colector originally started developing at [Treasure Data, Inc](http://www.treasure-data.com/). Fluentd is specifically designed for solving big data collection problem. A lot of users are using Fluentd with MongoDB to collect semi-structured data, but it doesn't scale well for now.

HDFS (Hadoop) is a natural alternative for storing and processing huge amount of big data, but it didn't have an accessible API interface other than Java library until recently. But from Apache 1.0.0, CDH3u5 or CDH4 or later, HDFS supports the interface called WebHDFS, which allows the users to operate with HTTP.

This post shows how to receive the data from HTTP, and streamingly put it into HDFS through WebHDFS interface with [Fluentd](http://fluentd.org/), with a little configuration.

## Install

For simplicity, this post shows the one-node configuration. You should have the following software installed on the same node.

* [Fluentd](http://fluentd.org/) with [WebHDFS Plugin](https://github.com/fluent/fluent-plugin-webhdfs/) ([out_webhdfs](out_webhdfs))
* HDFS (Apache 1.0.0, CDH3u5 or CDH4 or later)

Fluentd’s most recent version of deb/rpm package (v1.1.10 or later) includes the WebHDFS plugin. If you want to use Ruby Gems to install the plugin, `gem install fluent-plugin-webhdfs` does the job.

* [Debian Package](install-by-deb)
* [RPM Package](install-by-rpm)
* For CDH, please refer to the [downloads page](https://ccp.cloudera.com/display/SUPPORT/CDH+Downloads) (CDH3u5 and CDH4 or later)

## Fluentd Configuration

Let’s start the actual configurations. If you use deb/rpm, the Fluentd’s config file is located at /etc/td-agent/td-agent.conf. Otherwise, it is located at /etc/fluentd/fluentd.conf.

### HTTP Input

For input, let's setup Fluentd to accept the record from HTTP. This is what the Fluentd configuration looks like.

    <source>
      type http
      port 8080
    </source>

### WebHDFS Output

The output configuration should look like this:

    <match hdfs.access.**>
      type webhdfs
      host namenode.your.cluster.local
      port 50070
      path /log/%Y%m%d_%H/access.log.${hostname}
      flush_interval 10s
    </match>

The match section specifies the regexp to match the tags. If the tag is matched, then the config inside it is used.

**flush_internal** indicates how often the data is written to the HDFS. Append operation is used to append the in coming data to the file specified by **path** parameter.

At path parameter, you can use the place holder for time and also hostname. This prevents multiple Fluentd instances to append the data into the single file, which must be avoided for append operation.

Other options specify HDFS’s NameNode host and port.

## HDFS Configuration

Append operations is not enabled by default. Please put these configurations into your hdfs-site.xml, and restart the whole cluster.

    <property>
      <name>dfs.webhdfs.enabled</name>
      <value>true</value>
    </property>
    
    <property>
      <name>dfs.support.append</name>
      <value>true</value>
    </property>
    
    <property>
      <name>dfs.support.broken.append</name>
      <value>true</value>
    </property>

Also, please be careful that *path* specified at WebHDFS output is configured to be writable by hdfs user.

## Test

To test the configuration, just post the JSON to Fluentd. This example users curl command to do so.

    $ curl -X POST -d 'json={"action":"login","user":2}' \
      http://localhost:8080/hdfs.access.test

Then, let’s access HDFS and see the stored data.

    $ sudo -u hdfs hadoop fs -lsr /log/
    drwxr-xr-x   - 1 supergroup          0 2012-10-22 09:40 /log/20121022_14/access.log.dev

## Conclusion

Fluentd + WebHDFS make real-time log collection simple, easy, robust and scalable! [@tagomoris](http://github.com/tagomoris) already uses this plugin to collect 20,000 msgs/sec, 1.5 TB/day without no major problem for couple of months.

## Further Readings

* [out_webhdfs](out_webhdfs)
* [Slide: Fluentd and WebHDFS](http://www.slideshare.net/tagomoris/fluentd-and-webhdfs)
* [Fluentd Plugins List](http://fluentd.org/plugin/)
* [Fluentd Source Code](http://github.com/fluent/fluentd/)
