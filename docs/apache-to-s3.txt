# Store Apache Logs into Amazon S3

This article shows how to use [Fluentd](http://fluentd.org/)'s Amazon S3 plugin ([out_s3](out_s3)) to aggregate semi-structured logs in real-time.

## Background

[Fluentd](http://fluentd.org/) is an advanced open-source log collector developed at [Treasure Data, Inc](http://www.treasure-data.com/). One of the major purpose of aggregating logs is data archiving. [Amazon S3](http://aws.amazon.com/s3/), the cloud object storage provided by Amazon is widely used for the data archiving.

This post shows how to import Apache logs into Amazon S3 with Fluentd, by really small configurations.

## Mechanism

Fluentd does 3 things:

1. It continuously “tails” the access log.
2. It parses the incoming log entries into meaningful fields (such as `ip`, `path`, etc) and buffers them.
3. It writes the buffered data to Amazon S3 periodically.

## Install

For simplicity, this post shows the one-node configuration. You should have the following software installed on the same node.

* Fluentd with Amazon S3 Plugin
* Your Amazon Web Services Account
* Apache (with the Combined Log Format)

Fluentd’s most recent version of deb/rpm package includes the Amazon S3 plugin. If you want to use Ruby Gems to install the plugin, `gem install fluent-plugin-s3` does the job.

* [Debian Package](install-by-deb)
* [RPM Package](install-by-rpm)
* [Ruby gem](install-by-gem)

## Configuration

Let's start the actual configurations. If you use deb/rpm, the Fluentd’s config file is located at `/etc/td-agent/td-agent.conf`. Otherwise, it is located at `/etc/fluentd/fluentd.conf`.

### Tail Input

For input, let's set up Fluentd to track the recent Apache logs (usually at /var/log/apache2/access_log). This is what the Fluentd configuration looks like.

    :::text
    <source>
      type tail
      format apache
      path /var/log/apache2/access_log
      tag s3.apache.access
    </source>

Let’s go through the configuration line by line.

1. `type tail`: The tail plugin continuously tracks the log file. This handy plugin is part of Fluentd’s core plugins.
2. `format apache`: Use Fluentd’s built-in Apache log parser.
3. `path /var/log/apache2/access_log`: Assuming the Apache log is in `/var/log/apache2/access_log`.
4. `tag s3.apache.access`: `s3.apache.access` is used as tag to route the message at Fluentd.

That’s it. You should be able to output a JSON-formatted data stream for Fluentd to consume.

### Amazon S3 Output

The output configuration should look like this:

    :::text
    <match s3.*.*>
      type s3

      aws_key_id YOUR_AWS_KEY_ID
      aws_sec_key YOUR_AWS_SECRET/KEY
      s3_bucket YOUR_S3_BUCKET_NAME
      path logs/
      buffer_path /var/log/fluent/s3

      time_slice_format %Y%m%d-%H
      time_slice_wait 10m
      utc
    </match>

The `match` section specifies the regexp to match the tags. If the tag is matched, then the config inside the `<match>...</match>` is used. In this example, the `s3.apache.access` tag (generated by `tail`) is always used.

## Test
To test the configuration, just ping the Apache server. This example uses `ab` (Apache Bench) program.

    :::term
    $ ab -n 100 -c 10 http://localhost/

Then, please login into [AWS Console](https://console.aws.amazon.com/s3/home) and look at your bucket.

WARNING: By default, Fluentd uploads the data into Amazon S3 `EVERY HOUR` (around xx:10). Please be patient about it. If you want to upload minutely, please change `time_slice_format` parameter to `%Y%m%d-%H%M`.

## Conclusion
Fluentd + Amazon S3 makes log archiving fairly simple.

## Further Readings
- [out_s3](out_s3)
- [Fluentd Plugins List](http://fluentd.org/plugin/)
- [Fluentd Source Code](https://github.com/fluent/fluentd)
