# Free Alternative to Splunk Using Fluentd

[Splunk](http://www.splunk.com/) is a great tool for searching logs, but its high cost makes it prohibitive for many teams. In this article, we present a free and open source alternative to Splunk by combining three open source projects: Elasticsearch, Kibana, and Fluentd.

<img src="/images/fluentd-elasticsearch-kibana.png"/>

[Elasticsearch](http://www.elasticsearch.org/) is an open source search engine known for its ease of use. [Kibana](http://kibana.org/) is an open source Web UI that makes Elasticsearch user friendly for marketers, engineers and data scientists alike.

Kibana supports the LogStash format, which in turn is supported by the fluentd-plugin-elasticsearch plugin. And of course, Fluentd is the open source log collector known for its scalability and flexibility.

By combining these three tools we get a scalable, flexible, easy to use log search engine with a great Web UI that provides an end to end solution from log collection to visualization, all for free!

In this guide, we will go over installation, setup, and basic use of this combined log search solution. The contents of this article were tested on Mac OS X Mountain Lion.

## Get Up and Running in 2 Minutes

### One Line to Install Them All

If you are new to both Fluentd and Elasticsearch/Kibana, no worries. Run:

    $ curl -Ls http://docs.fluentd.org/misc/fluentd-kibana-elasticsearch/setup.sh | bash

and you are good to go (**make sure you have write access to the current directory**). Essentially, that command creates a Fluentd/Elasticsearch/Kibana "sandbox" right on your machine to test what the integratoin looks like (See the diagram below)

<img src="/images/fluentd-elasticsearch-kibana-all-in-one.png"/>

### Send Data to Elasticsearch and Visuazlize in Kibana!

In the same directory, launch Fluentd, Elasticsearch and Kibana by running the following command:

    $ fluentd -c test.conf

* This Fluentd instance has been configured to accept HTTP messages on port 8888. Go visit `http://localhost:8888/es/test?json={"message":"hello world!"}` This sends a JSON event `{"message":"hello world!"}` with the tag `es.test`. Fluentd matches this event with the `<match es.**>...</match>` statement and sends it to Elasticsearch.
* Check the data on Kibana by visiting `http://localhost:24300/kibana/index.html#/dashboard/file/guided.json`!

That's it! If you are interested in setting this up for production, read along.

<img src="/images/kibana-screenshot.png"/>

## In Production Setup

In this section, we assume you are already familiar with Elasticsearch and Kibana and **have an Elasticsearch+Kibana instance running somewhere**.

### Set Up Treasure Agent (enterprise counterpart for Fluentd)

In this guide We'll install Treasure Agent, the QA'ed counterpart of Fluentd supported by [Treasure Data](http://www.treasuredata.com/en/products/treasure-agent.php). Please refer to the guides below for detailed installation steps.

* [Debian Package](install-by-deb)
* [RPM Package](install-by-rpm)

Next, we'll install the Elasticsearch plugin for Fluentd: fluent-plugin-elasticsearch.

    :::term
    $ /usr/lib64/fluent/ruby/bin/fluent-gem install fluent-plugin-elasticsearch

We'll configure td-agent (Fluentd) to interface properly with Elasticsearch. Please modify `/etc/td-agent/td-agent.conf` as shown below:

    :::text
    #Comments are preceded with '#' like Perl/Python/Ruby

    <source> #Input plugins are defined be <source>...</source> statements
      type syslog #This is the Syslog input plugin, which we will use later in this tutorial
      port 42185
      tag es.syslog

    <source>
      type http #This is the HTTP input plugin, which turns Fluentd into an HTTP endpoint
      port 8888 #Send data to http://localhost:8888/<Fluentd tag>?json=<json event>
    </source>
    
    <match es.**>
      type elasticsearch
      logstash_format true
      index_name fluentd
      type_name fluentd
      flush_interval 3 # For testing
      host <YOUR ELASTICSEARCH URL>
      port <YOUR ELASTICSEARCH PORT>
    </match>

Once everything has been set up and configured, we'll start td-agent.

    :::term
    $ sudo /etc/init.d/td-agent start

### Got rsyslogd?

In our final step, we'll forward the logs from your rsyslogd to Fluentd. Please add the following line to your `/etc/rsyslog.conf`, and restart rsyslog. This will forward your local syslog to Fluentd, and Fluentd in turn will forward the logs to Elasticsearch.

    :::text
    *.* @127.0.0.1:42185

Please restart the rsyslog service once the modification is complete.

    :::text
    $ sudo /etc/init.d/rsyslog restart

Now, your syslog data should be flowing through Fluentd into Elasticsearch.

To manually send logs to Elasticsearch, please use the `logger` command.

    :::text
    $ logger -t test foobar

When debugging your td-agent configuration, using [out_copy](out_copy) + [out_stdout](out_stdout) will be useful. All the logs including errors can be found at `/etc/td-agent/td-agent.log`.

    :::text
    <match es.**>
      type copy
      <store>
        # for debug (see /var/log/td-agent.log)
        type stdout
      </store>
      <store>
        type elasticsearch
        logstash_format true
        index_name fluentd
        type_name fluentd
        flush_interval 3 # For testing
        host <YOUR ELASTICSEARCH URL>
        port <YOUR ELASTICSEARCH PORT>
      </store>
    </match>

## Conclusion

This article introduced the combination of Fluentd and Kibana (with Elasticsearch) which achieves a free alternative to Splunk: storing and searching machine logs. The examples provided in this article have not been tuned.

If you will be using these components in production, you may want to modify some of the configurations (e.g. JVM, Elasticsearch, Fluentd buffer, etc.) according to your needs.
